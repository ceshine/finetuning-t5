{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate SentencePiece Vocabulary\n",
    "\n",
    "## References\n",
    "\n",
    "* [add new vocab](https://github.com/google/sentencepiece/blob/9cf136582d9cce492ba5a0cfb775f9e777fe07ea/python/add_new_vocab.ipynb) from google/sentencepiece\n",
    "* [reduce vocab](https://github.com/bojone/t5_in_bert4keras/blob/6cf50dbf3ffd3b4e9f36a59ee9f98356cf686de0/tokenizer/reduce.py) from bojone/t5_in_bert4keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a pretrained tokenizer (mT5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "Path(\"cache/\").mkdir(exist_ok=True)\n",
    "if Path(\"cache/mt5-small\").exists():\n",
    "    shutil.rmtree(\"cache/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5Tokenizer\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cache/mt5-small/tokenizer_config.json',\n",
       " 'cache/mt5-small/special_tokens_map.json',\n",
       " 'cache/mt5-small/spiece.model',\n",
       " 'cache/mt5-small/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"cache/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Dataset (XNLI)\n",
    "\n",
    "We want to retain only pieces that are used in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xnli (/home/ceshine/.cache/huggingface/datasets/xnli/zh/1.1.0/51ba3a1091acf33fd7c2a54bcbeeee1b1df3ecb127fdca003d31968fa3a1e6a8)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xnli\", \"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['产品 和 地理 是 什么 使 奶油 抹 霜 工作 .',\n",
       " '如果 人们 记得 的 话 , 你 就 会 把 事情 弄 丢 了 .',\n",
       " '我 团队 的 一个 成员 将 非常 精确 地 执行 你 的 命令',\n",
       " '这些 信息 属于 他们 .',\n",
       " '网球鞋 有 一 系列 的 价格 .',\n",
       " '我 很 难 过 我 的 随身听 坏 了 现在 我 得 把 音响 调 大 一点',\n",
       " '大多数 基督教 马赛克 都 被 穆斯林 摧毁 .',\n",
       " '石板 对 杰克逊 的 调查 结果 有 意见',\n",
       " '异性恋者',\n",
       " '孚日 广场 完全 是 用 灰色 大理石 建造 的 .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"hypothesis\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[259, 15104, 259, 1107, 259, 148479, 259, 1543, 259, 16892, 259, 12561, 259, 63749, 10920, 259, 126767, 259, 155228, 259, 6573, 259, 260, 1], [259, 21304, 259, 79316, 259, 177378, 259, 493, 259, 30253, 259, 261, 259, 4235, 259, 3981, 259, 2219, 259, 9803, 259, 52597, 259, 91253, 259, 142089, 259, 1322, 259, 260, 1], [259, 3003, 259, 61105, 259, 493, 259, 8149, 259, 98581, 259, 3661, 259, 25265, 259, 12348, 107310, 259, 2524, 259, 55958, 259, 4235, 259, 493, 259, 129300, 1], [259, 20155, 259, 12359, 259, 80922, 259, 16171, 259, 260, 1], [259, 1758, 8320, 62043, 259, 1637, 259, 1374, 259, 27858, 259, 493, 259, 21919, 259, 260, 1], [259, 3003, 259, 10559, 259, 20481, 259, 6994, 259, 3003, 259, 493, 259, 24470, 7431, 24762, 259, 90707, 259, 1322, 259, 24150, 259, 3003, 259, 5880, 259, 9803, 259, 7647, 42797, 259, 19477, 259, 1146, 259, 39200, 1], [259, 155598, 259, 16746, 55627, 11072, 259, 6890, 16003, 9636, 259, 4794, 259, 3916, 259, 131330, 9684, 6892, 259, 234510, 113286, 259, 260, 1], [259, 6058, 8844, 259, 2991, 259, 58174, 9636, 168402, 259, 493, 259, 45849, 259, 34806, 259, 1637, 259, 94238, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(dataset[\"train\"][\"hypothesis\"][:8], return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize_data(data, batch_size=1024):\n",
    "    global seen\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        seen = seen.union(\n",
    "            set(chain.from_iterable(tokenizer.batch_encode_plus(data[i:(i+batch_size)], return_attention_mask=False)[\"input_ids\"]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/384 [00:00<00:27, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [00:25<00:00, 14.95it/s]\n",
      "100%|██████████| 384/384 [00:43<00:00,  8.85it/s]\n",
      " 40%|████      | 2/5 [00:00<00:00, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.68it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.53it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.82it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 16.88it/s]\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "for subset in (\"train\", \"test\", \"validation\"):\n",
    "    print(subset)\n",
    "    tokenize_data(dataset[subset][\"hypothesis\"])\n",
    "    tokenize_data(dataset[subset][\"premise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add some additional (meta) tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = seen.union(set(tokenizer.encode(\"mnli premise: hypothesis: <unk>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30316"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the SentencePiece Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <pad>\n",
      "20 <0x11>\n",
      "40 <0x25>\n",
      "60 <0x39>\n",
      "80 <0x4D>\n",
      "100 <0x61>\n",
      "120 <0x75>\n",
      "140 <0x89>\n",
      "160 <0x9D>\n",
      "180 <0xB1>\n",
      "200 <0xC5>\n",
      "220 <0xD9>\n",
      "240 <0xED>\n",
      "260 .\n",
      "280 l\n",
      "300 ▁v\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import sentencepiece_model_pb2 as model\n",
    "\n",
    "m = model.ModelProto()\n",
    "m.ParseFromString(open(\"cache/mt5-small/spiece.model\", 'rb').read())\n",
    "# There are some reserved places for speical tokens\n",
    "for i, piece in enumerate(m.pieces[:320]):\n",
    "    if i % 20 == 0:\n",
    "        print(i, piece.piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<0xFF>', '▁')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces[258].piece, m.pieces[259].piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrink the SentencePiece Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30513"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_pieces, i = [], len(m.pieces) - 1\n",
    "while len(m.pieces):\n",
    "    piece = m.pieces.pop()\n",
    "    if i < 259 or i in seen:\n",
    "        kept_pieces.append(piece)\n",
    "    i -= 1\n",
    "kept_pieces = list(reversed(kept_pieces))\n",
    "len(kept_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30513"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pieces.extend(kept_pieces)\n",
    "len(m.pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup the old model and save the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"cache/mt5-small/spiece.model\").rename(\"cache/mt5-small/spiece.model.old\")\n",
    "with open(\"cache/mt5-small/spiece.model\", 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to save the list of ids that are kept to trim the embedding matrix later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30513\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "kept_ids = sorted(list(seen.union(set(range(259)))))\n",
    "print(len(kept_ids))\n",
    "with open(\"cache/mt5-small/kept_ids.json\", 'w') as f:\n",
    "    json.dump(kept_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First test the dumped `kept_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30513"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"cache/mt5-small/kept_ids.json\") as f:\n",
    "    tmp = json.load(f)\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4], [249716, 249738, 249740, 249753, 249834])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:5], tmp[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"cache/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'产品 和 地理 是 什么 使 奶油 抹 霜 工作.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(dataset[\"train\"][\"hypothesis\"][0]), skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few more, just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in random.sample(range(100), k=10):\n",
    "    converted = tokenizer.decode(\n",
    "        tokenizer.encode(dataset[\"train\"][\"hypothesis\"][i]), skip_special_tokens=True\n",
    "    ).replace(\" \", \"\") # the space placements are slightly different from the original\n",
    "    assert converted == dataset[\"train\"][\"hypothesis\"][i].replace(\" \", \"\"), f'{converted}\\n{dataset[\"train\"][\"hypothesis\"][i]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
